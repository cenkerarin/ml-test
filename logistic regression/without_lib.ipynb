{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ed1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a42a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/cenkerarin/ml_test/logistic regression/train_clean.csv')\n",
    "test = pd.read_csv('/Users/cenkerarin/ml_test/logistic regression/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1693ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>has_cabin_letter</th>\n",
       "      <th>ticket_has_letter</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  \\\n",
       "0            1         0       3  22.0      1      0   7.2500   \n",
       "1            2         1       1  38.0      1      0  71.2833   \n",
       "2            3         1       3  26.0      0      0   7.9250   \n",
       "3            4         1       1  35.0      1      0  53.1000   \n",
       "4            5         0       3  35.0      0      0   8.0500   \n",
       "\n",
       "   has_cabin_letter  ticket_has_letter  Sex_male  Embarked_Q  Embarked_S  \n",
       "0                 0                  1       1.0         0.0         1.0  \n",
       "1                 1                  1       0.0         0.0         0.0  \n",
       "2                 0                  1       0.0         0.0         1.0  \n",
       "3                 1                  0       0.0         0.0         1.0  \n",
       "4                 0                  0       1.0         0.0         1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d36de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression from Scratch ===\n",
      "Training Data:\n",
      "Features shape: (891, 10)\n",
      "Labels shape: (891,)\n",
      "\n",
      "Training model...\n",
      "Iteration 0, Cost: 0.630413\n",
      "Iteration 100, Cost: 0.833880\n",
      "Iteration 200, Cost: 0.739820\n",
      "Iteration 300, Cost: 1.398218\n",
      "Iteration 400, Cost: 0.750975\n",
      "Iteration 500, Cost: 0.687159\n",
      "Iteration 600, Cost: 1.364983\n",
      "Iteration 700, Cost: 0.710325\n",
      "Iteration 800, Cost: 0.670111\n",
      "Iteration 900, Cost: 1.263720\n",
      "\n",
      "Model weights (including bias): [0.071817397736894, -0.22660142155241841, -0.10097097079170185, -0.5264778756862752, -0.09108494739278873, 0.0338064690346868, 0.24370001618649387, -0.06319262111473713, -0.9020012275268127, 0.07311546116849305, -0.1449035587061667]\n",
      "\n",
      "Test predictions (first 10 samples):\n",
      "Sample 1: Probability: 0.0094, Prediction: 0\n",
      "Sample 2: Probability: 0.0031, Prediction: 0\n",
      "Sample 3: Probability: 0.0008, Prediction: 0\n",
      "Sample 4: Probability: 0.0165, Prediction: 0\n",
      "Sample 5: Probability: 0.0401, Prediction: 0\n",
      "Sample 6: Probability: 0.0597, Prediction: 0\n",
      "Sample 7: Probability: 0.0354, Prediction: 0\n",
      "Sample 8: Probability: 0.0243, Prediction: 0\n",
      "Sample 9: Probability: 0.1015, Prediction: 0\n",
      "Sample 10: Probability: 0.0167, Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression from Scratch - Educational Implementation\n",
    "# This implementation uses only basic Python to understand the underlying mathematics\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term to feature matrix\"\"\"\n",
    "        # Convert pandas DataFrame to list of lists if necessary\n",
    "        if hasattr(X, 'values'):\n",
    "            X_list = X.values.tolist()\n",
    "        else:\n",
    "            X_list = X\n",
    "        \n",
    "        bias_column = [1.0] * len(X_list)\n",
    "        return [[bias_column[i]] + X_list[i] for i in range(len(X_list))]\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = max(-500, min(500, z))\n",
    "        return 1.0 / (1.0 + (2.718281828 ** (-z)))\n",
    "    \n",
    "    def _predict_proba_single(self, x):\n",
    "        \"\"\"Predict probability for a single sample\"\"\"\n",
    "        z = sum(w * feature for w, feature in zip(self.weights, x))\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def _cost_function(self, X, y):\n",
    "        \"\"\"Calculate logistic regression cost function\"\"\"\n",
    "        total_cost = 0.0\n",
    "        m = len(X)\n",
    "        \n",
    "        # Convert y to list if it's a pandas Series\n",
    "        if hasattr(y, 'values'):\n",
    "            y_list = y.values.tolist()\n",
    "        else:\n",
    "            y_list = y\n",
    "        \n",
    "        for i in range(m):\n",
    "            h = self._predict_proba_single(X[i])\n",
    "            # Add small epsilon to prevent log(0)\n",
    "            epsilon = 1e-15\n",
    "            h = max(epsilon, min(1 - epsilon, h))\n",
    "            \n",
    "            cost = -(y_list[i] * self._log(h) + (1 - y_list[i]) * self._log(1 - h))\n",
    "            total_cost += cost\n",
    "            \n",
    "        return total_cost / m\n",
    "    \n",
    "    def _log(self, x):\n",
    "        \"\"\"Natural logarithm approximation\"\"\"\n",
    "        # Simple approximation for natural log\n",
    "        if x <= 0:\n",
    "            return -1000  # Very negative number for log(0)\n",
    "        \n",
    "        # Use the fact that ln(x) = 2 * (y + y^3/3 + y^5/5 + ...) where y = (x-1)/(x+1)\n",
    "        if x == 1:\n",
    "            return 0\n",
    "        \n",
    "        y = (x - 1) / (x + 1)\n",
    "        y_squared = y * y\n",
    "        result = y\n",
    "        term = y\n",
    "        \n",
    "        for i in range(1, 10):  # Use first 10 terms for approximation\n",
    "            term *= y_squared\n",
    "            result += term / (2 * i + 1)\n",
    "            \n",
    "        return 2 * result\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model\"\"\"\n",
    "        # Add bias term to features\n",
    "        X_with_bias = self._add_bias(X)\n",
    "        \n",
    "        # Convert y to list if it's a pandas Series\n",
    "        if hasattr(y, 'values'):\n",
    "            y_list = y.values.tolist()\n",
    "        else:\n",
    "            y_list = y\n",
    "        \n",
    "        # Initialize weights (including bias)\n",
    "        num_features = len(X_with_bias[0])\n",
    "        self.weights = [0.0] * num_features\n",
    "        \n",
    "        # Gradient descent\n",
    "        m = len(X_with_bias)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Calculate gradients\n",
    "            gradients = [0.0] * num_features\n",
    "            \n",
    "            for i in range(m):\n",
    "                h = self._predict_proba_single(X_with_bias[i])\n",
    "                error = h - y_list[i]\n",
    "                \n",
    "                for j in range(num_features):\n",
    "                    gradients[j] += error * X_with_bias[i][j]\n",
    "            \n",
    "            # Update weights\n",
    "            for j in range(num_features):\n",
    "                self.weights[j] -= self.learning_rate * (gradients[j] / m)\n",
    "            \n",
    "            # Print cost every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                cost = self._cost_function(X_with_bias, y_list)\n",
    "                print(f\"Iteration {iteration}, Cost: {cost:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities for samples\"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        X_with_bias = self._add_bias(X)\n",
    "        probabilities = []\n",
    "        \n",
    "        for sample in X_with_bias:\n",
    "            prob = self._predict_proba_single(sample)\n",
    "            probabilities.append(prob)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return [1 if prob >= threshold else 0 for prob in probabilities]\n",
    "\n",
    "# Example usage with simple data\n",
    "print(\"=== Logistic Regression from Scratch ===\")\n",
    "\n",
    "# Create simple dataset for demonstration\n",
    "# Features: [hours_studied, previous_score]\n",
    "X_simple = train.drop(columns=['Survived', 'PassengerId'])\n",
    "\n",
    "# Target: pass (1) or fail (0)\n",
    "y_simple = train['Survived']\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(\"Features shape:\", X_simple.shape)\n",
    "print(\"Labels shape:\", y_simple.shape)\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(learning_rate=0.01, max_iterations=1000)\n",
    "print(\"Training model...\")\n",
    "model.fit(X_simple, y_simple)\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "print(\"Model weights (including bias):\", model.weights)\n",
    "print()\n",
    "\n",
    "# Test predictions\n",
    "X_test = test.drop(columns=['PassengerId'])\n",
    "probabilities = model.predict_proba(X_test)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Test predictions (first 10 samples):\")\n",
    "for i in range(min(10, len(X_test))):\n",
    "    print(f\"Sample {i+1}: Probability: {probabilities[i]:.4f}, Prediction: {predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e75cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission dataframe:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         0\n",
      "\n",
      "Submission shape: (418, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "\n",
    "print(\"Submission dataframe:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "submission.to_csv('submission_without_lib.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b61fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
